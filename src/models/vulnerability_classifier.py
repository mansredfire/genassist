"""Vulnerability type classifier - Production Implementation"""

import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Optional
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.metrics import precision_recall_fscore_support, roc_auc_score
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier
import pickle
import json
from pathlib import Path


class VulnerabilityPredictor:
    """
    Multi-class classifier to predict vulnerability types
    
    Uses ensemble of:
    - Random Forest
    - XGBoost
    - LightGBM
    - CatBoost
    - Gradient Boosting
    
    Supports multiple ensemble methods:
    - Voting (majority vote)
    - Averaging (average probabilities)
    - Weighted (weighted by validation performance)
    """
    
    def __init__(self, model_type: str = 'ensemble', random_state: int = 42):
        self.model_type = model_type
        self.random_state = random_state
        self.models = {}
        self.feature_importance = {}
        self.class_labels = None
        self.validation_scores = {}
        self.is_trained = False
    
    def build_models(self):
        """Build all models for ensemble"""
        
        print("Building ensemble models...")
        
        self.models = {
            'random_forest': RandomForestClassifier(
                n_estimators=200,
                max_depth=15,
                min_samples_split=5,
                min_samples_leaf=2,
                max_features='sqrt',
                class_weight='balanced',
                random_state=self.random_state,
                n_jobs=-1,
                verbose=0
            ),
            
            'xgboost': xgb.XGBClassifier(
                n_estimators=200,
                max_depth=10,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                gamma=0.1,
                reg_alpha=0.1,
                reg_lambda=1.0,
                random_state=self.random_state,
                n_jobs=-1,
                verbosity=0
            ),
            
            'lightgbm': lgb.LGBMClassifier(
                n_estimators=200,
                max_depth=10,
                learning_rate=0.1,
                num_leaves=31,
                subsample=0.8,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=1.0,
                random_state=self.random_state,
                n_jobs=-1,
                verbose=-1
            ),
            
            'catboost': CatBoostClassifier(
                iterations=200,
                depth=10,
                learning_rate=0.1,
                l2_leaf_reg=3.0,
                random_state=self.random_state,
                verbose=False,
                thread_count=-1
            ),
            
            'gradient_boosting': GradientBoostingClassifier(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                subsample=0.8,
                max_features='sqrt',
                random_state=self.random_state,
                verbose=0
            )
        }
        
        print(f"Built {len(self.models)} models")
    
    def train(self, X: pd.DataFrame, y: pd.Series, 
              test_size: float = 0.2,
              validation_size: float = 0.1,
              perform_cv: bool = True) -> Dict:
        """
        Train all models and return comprehensive metrics
        
        Args:
            X: Feature dataframe
            y: Target variable (vulnerability types)
            test_size: Proportion of data for testing
            validation_size: Proportion of training data for validation
            perform_cv: Whether to perform cross-validation
            
        Returns:
            Dictionary with training results and metrics
        """
        
        print(f"\nTraining VulnerabilityPredictor on {len(X)} samples...")
        print(f"Features: {X.shape[1]}")
        print(f"Classes: {y.nunique()}")
        
        # Store class labels
        self.class_labels = sorted(y.unique())
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, 
            test_size=test_size, 
            random_state=self.random_state,
            stratify=y
        )
        
        # Further split training into train and validation
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train,
            test_size=validation_size,
            random_state=self.random_state,
            stratify=y_train
        )
        
        print(f"Train set: {len(X_train)} samples")
        print(f"Validation set: {len(X_val)} samples")
        print(f"Test set: {len(X_test)} samples")
        
        # Build models if not already built
        if not self.models:
            self.build_models()
        
        results = {}
        
        # Train each model
        for name, model in self.models.items():
            print(f"\n{'='*60}")
            print(f"Training {name}...")
            print(f"{'='*60}")
            
            try:
                # Train
                model.fit(X_train, y_train)
                
                # Predictions
                y_train_pred = model.predict(X_train)
                y_val_pred = model.predict(X_val)
                y_test_pred = model.predict(X_test)
                
                # Scores
                train_score = accuracy_score(y_train, y_train_pred)
                val_score = accuracy_score(y_val, y_val_pred)
                test_score = accuracy_score(y_test, y_test_pred)
                
                # F1 scores
                train_f1 = f1_score(y_train, y_train_pred, average='weighted')
                val_f1 = f1_score(y_val, y_val_pred, average='weighted')
                test_f1 = f1_score(y_test, y_test_pred, average='weighted')
                
                print(f"Train Accuracy: {train_score:.4f} | F1: {train_f1:.4f}")
                print(f"Val Accuracy:   {val_score:.4f} | F1: {val_f1:.4f}")
                print(f"Test Accuracy:  {test_score:.4f} | F1: {test_f1:.4f}")
                
                # Cross-validation
                cv_scores = None
                if perform_cv:
                    print("Performing 5-fold cross-validation...")
                    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
                    cv_scores = cross_val_score(
                        model, X_train, y_train, 
                        cv=cv, 
                        scoring='f1_weighted',
                        n_jobs=-1
                    )
                    print(f"CV F1 Score: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
                
                # Store validation score for ensemble weighting
                self.validation_scores[name] = val_f1
                
                # Feature importance
                if hasattr(model, 'feature_importances_'):
                    self.feature_importance[name] = dict(zip(
                        X.columns, 
                        model.feature_importances_
                    ))
                
                # Classification report
                test_report = classification_report(
                    y_test, y_test_pred,
                    target_names=[str(c) for c in self.class_labels],
                    output_dict=True
                )
                
                # Confusion matrix
                conf_matrix = confusion_matrix(y_test, y_test_pred)
                
                # Store results
                results[name] = {
                    'train_accuracy': train_score,
                    'val_accuracy': val_score,
                    'test_accuracy': test_score,
                    'train_f1': train_f1,
                    'val_f1': val_f1,
                    'test_f1': test_f1,
                    'cv_scores': cv_scores.tolist() if cv_scores is not None else None,
                    'cv_mean': cv_scores.mean() if cv_scores is not None else None,
                    'cv_std': cv_scores.std() if cv_scores is not None else None,
                    'classification_report': test_report,
                    'confusion_matrix': conf_matrix.tolist()
                }
                
                print(f"✓ {name} training complete")
                
            except Exception as e:
                print(f"✗ Error training {name}: {e}")
                results[name] = {'error': str(e)}
        
        self.is_trained = True
        
        # Print ensemble summary
        print(f"\n{'='*60}")
        print("ENSEMBLE SUMMARY")
        print(f"{'='*60}")
        for name, metrics in results.items():
            if 'error' not in metrics:
                print(f"{name:20s} - Test F1: {metrics['test_f1']:.4f}")
        
        return results
    
    def predict(self, X: pd.DataFrame, model_name: Optional[str] = None) -> np.ndarray:
        """
        Make predictions using a specific model or ensemble
        
        Args:
            X: Feature dataframe
            model_name: Specific model to use, or None for ensemble
            
        Returns:
            Predicted class labels
        """
        
        if not self.is_trained:
            raise ValueError("Models must be trained before prediction")
        
        if model_name:
            if model_name not in self.models:
                raise ValueError(f"Model {model_name} not found")
            return self.models[model_name].predict(X)
        else:
            # Use ensemble
            predictions, _ = self.ensemble_predict(X, method='averaging')
            return predictions
    
    def predict_probabilities(self, X: pd.DataFrame) -> Dict[str, np.ndarray]:
        """
        Get probability predictions from all models
        
        Args:
            X: Feature dataframe
            
        Returns:
            Dictionary mapping model names to probability arrays
        """
        
        if not self.is_trained:
            raise ValueError("Models must be trained before prediction")
        
        probabilities = {}
        
        for name, model in self.models.items():
            if hasattr(model, 'predict_proba'):
                probabilities[name] = model.predict_proba(X)
        
        return probabilities
    
    def ensemble_predict(self, X: pd.DataFrame, 
                         method: str = 'averaging') -> Tuple[np.ndarray, np.ndarray]:
        """
        Ensemble prediction using multiple methods
        
        Args:
            X: Feature dataframe
            method: Ensemble method ('voting', 'averaging', 'weighted')
            
        Returns:
            Tuple of (predictions, probabilities)
        """
        
        if not self.is_trained:
            raise ValueError("Models must be trained before prediction")
        
        all_probs = self.predict_probabilities(X)
        
        if not all_probs:
            raise ValueError("No models with probability predictions available")
        
        if method == 'voting':
            # Majority voting
            predictions = []
            for model in self.models.values():
                predictions.append(model.predict(X))
            predictions = np.array(predictions)
            
            # Get majority vote for each sample
            final_pred = np.array([
                np.bincount(predictions[:, i]).argmax()
                for i in range(predictions.shape[1])
            ])
            
            final_probs = None
            
        elif method == 'averaging':
            # Average probabilities
            probs_stack = np.array(list(all_probs.values()))
            final_probs = np.mean(probs_stack, axis=0)
            final_pred = np.argmax(final_probs, axis=1)
            
        elif method == 'weighted':
            # Weighted average based on validation performance
            weights = self._calculate_weights()
            weighted_probs = np.zeros_like(list(all_probs.values())[0])
            
            for name, probs in all_probs.items():
                weighted_probs += weights[name] * probs
            
            final_probs = weighted_probs
            final_pred = np.argmax(final_probs, axis=1)
            
        else:
            raise ValueError(f"Unknown ensemble method: {method}")
        
        return final_pred, final_probs
    
    def _calculate_weights(self) -> Dict[str, float]:
        """Calculate model weights based on validation performance"""
        
        if not self.validation_scores:
            # Equal weights if no validation scores
            n_models = len(self.models)
            return {name: 1.0 / n_models for name in self.models.keys()}
        
        # Softmax weighting based on validation F1 scores
        scores = np.array(list(self.validation_scores.values()))
        exp_scores = np.exp(scores - np.max(scores))  # Numerical stability
        weights_array = exp_scores / np.sum(exp_scores)
        
        weights = dict(zip(self.validation_scores.keys(), weights_array))
        
        return weights
    
    def get_feature_importance(self, top_n: int = 20, 
                               model_name: str = 'random_forest') -> pd.DataFrame:
        """
        Get top N most important features
        
        Args:
            top_n: Number of top features to return
            model_name: Which model's feature importance to use
            
        Returns:
            DataFrame with feature names and importance scores
        """
        
        if model_name not in self.feature_importance:
            raise ValueError(f"Feature importance not available for {model_name}")
        
        importance_dict = self.feature_importance[model_name]
        
        # Sort by importance
        sorted_features = sorted(
            importance_dict.items(), 
            key=lambda x: x[1], 
            reverse=True
        )[:top_n]
        
        return pd.DataFrame(sorted_features, columns=['Feature', 'Importance'])
    
    def evaluate(self, X_test: pd.DataFrame, y_test: pd.Series, 
                 method: str = 'averaging') -> Dict:
        """
        Comprehensive evaluation on test set
        
        Args:
            X_test: Test features
            y_test: Test labels
            method: Ensemble method to use
            
        Returns:
            Dictionary with evaluation metrics
        """
        
        # Get predictions
        y_pred, y_proba = self.ensemble_predict(X_test, method=method)
        
        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        # Per-class metrics
        precision, recall, f1_per_class, support = precision_recall_fscore_support(
            y_test, y_pred, average=None
        )
        
        # Classification report
        report = classification_report(
            y_test, y_pred,
            target_names=[str(c) for c in self.class_labels],
            output_dict=True
        )
        
        # Confusion matrix
        conf_matrix = confusion_matrix(y_test, y_pred)
        
        results = {
            'accuracy': accuracy,
            'f1_score': f1,
            'precision_per_class': precision.tolist(),
            'recall_per_class': recall.tolist(),
            'f1_per_class': f1_per_class.tolist(),
            'support_per_class': support.tolist(),
            'classification_report': report,
            'confusion_matrix': conf_matrix.tolist(),
            'class_labels': [str(c) for c in self.class_labels]
        }
        
        return results
    
    def save(self, filepath: str):
        """Save trained models to file"""
        
        save_path = Path(filepath)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        state = {
            'models': self.models,
            'feature_importance': self.feature_importance,
            'class_labels': self.class_labels,
            'validation_scores': self.validation_scores,
            'is_trained': self.is_trained,
            'model_type': self.model_type,
            'random_state': self.random_state
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(state, f)
        
        print(f"Saved VulnerabilityPredictor to {filepath}")
    
    @classmethod
    def load(cls, filepath: str):
        """Load trained models from file"""
        
        with open(filepath, 'rb') as f:
            state = pickle.load(f)
        
        predictor = cls(
            model_type=state['model_type'],
            random_state=state['random_state']
        )
        
        predictor.models = state['models']
        predictor.feature_importance = state['feature_importance']
        predictor.class_labels = state['class_labels']
        predictor.validation_scores = state['validation_scores']
        predictor.is_trained = state['is_trained']
        
        print(f"Loaded VulnerabilityPredictor from {filepath}")
        
        return predictor
    
    def predict_top_k(self, X: pd.DataFrame, k: int = 5, 
                      method: str = 'averaging') -> List[List[Tuple[str, float]]]:
        """
        Get top-k vulnerability predictions with probabilities
        
        Args:
            X: Feature dataframe
            k: Number of top predictions to return
            method: Ensemble method
            
        Returns:
            List of lists containing (vulnerability_type, probability) tuples
        """
        
        _, probs = self.ensemble_predict(X, method=method)
        
        if probs is None:
            raise ValueError("Probabilities not available for this ensemble method")
        
        results = []
        
        for sample_probs in probs:
            # Get top-k indices
            top_indices = np.argsort(sample_probs)[-k:][::-1]
            
            # Create list of (class, probability) tuples
            top_predictions = [
                (str(self.class_labels[idx]), float(sample_probs[idx]))
                for idx in top_indices
            ]
            
            results.append(top_predictions)
        
        return results
    
    def get_model_agreement(self, X: pd.DataFrame) -> np.ndarray:
        """
        Calculate agreement between models for each prediction
        
        Args:
            X: Feature dataframe
            
        Returns:
            Array of agreement scores (0-1) for each sample
        """
        
        predictions = []
        for model in self.models.values():
            predictions.append(model.predict(X))
        
        predictions = np.array(predictions)
        
        # Calculate agreement as proportion of models that agree with majority
        agreement_scores = []
        for i in range(predictions.shape[1]):
            votes = predictions[:, i]
            majority = np.bincount(votes).argmax()
            agreement = np.mean(votes == majority)
            agreement_scores.append(agreement)
        
        return np.array(agreement_scores)
