# src/models/vulnerability_classifier.py

import numpy as np
import pandas as pd
from typing import List, Dict, Tuple
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier

class VulnerabilityPredictor:
    """
    Multi-class classifier to predict vulnerability types
    """
    
    def __init__(self, model_type: str = 'ensemble'):
        self.model_type = model_type
        self.models = {}
        self.feature_importance = {}
    
    def build_models(self):
        """Build multiple models for ensemble"""
        
        self.models = {
            'random_forest': RandomForestClassifier(
                n_estimators=200,
                max_depth=15,
                min_samples_split=5,
                min_samples_leaf=2,
                class_weight='balanced',
                random_state=42
            ),
            
            'xgboost': xgb.XGBClassifier(
                n_estimators=200,
                max_depth=10,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42
            ),
            
            'lightgbm': lgb.LGBMClassifier(
                n_estimators=200,
                max_depth=10,
                learning_rate=0.1,
                num_leaves=31,
                random_state=42
            ),
            
            'catboost': CatBoostClassifier(
                iterations=200,
                depth=10,
                learning_rate=0.1,
                random_state=42,
                verbose=False
            ),
            
            'gradient_boosting': GradientBoostingClassifier(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                random_state=42
            ),
            
            'neural_network': MLPClassifier(
                hidden_layer_sizes=(256, 128, 64),
                activation='relu',
                solver='adam',
                learning_rate='adaptive',
                max_iter=500,
                random_state=42
            )
        }
    
    def train(self, X: pd.DataFrame, y: pd.Series, 
              test_size: float = 0.2) -> Dict:
        """Train all models and return metrics"""
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        results = {}
        
        for name, model in self.models.items():
            print(f"Training {name}...")
            
            # Train
            model.fit(X_train, y_train)
            
            # Evaluate
            train_score = model.score(X_train, y_train)
            test_score = model.score(X_test, y_test)
            
            # Cross-validation
            cv_scores = cross_val_score(model, X_train, y_train, cv=5)
            
            # Predictions
            y_pred = model.predict(X_test)
            
            results[name] = {
                'train_accuracy': train_score,
                'test_accuracy': test_score,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'classification_report': classification_report(y_test, y_pred),
                'confusion_matrix': confusion_matrix(y_test, y_pred)
            }
            
            # Feature importance
            if hasattr(model, 'feature_importances_'):
                self.feature_importance[name] = dict(zip(
                    X.columns, 
                    model.feature_importances_
                ))
            
            print(f"{name} - Test Accuracy: {test_score:.4f}")
        
        return results
    
    def predict_probabilities(self, X: pd.DataFrame) -> Dict[str, np.ndarray]:
        """Get probability predictions from all models"""
        
        probabilities = {}
        
        for name, model in self.models.items():
            if hasattr(model, 'predict_proba'):
                probabilities[name] = model.predict_proba(X)
        
        return probabilities
    
    def ensemble_predict(self, X: pd.DataFrame, 
                         method: str = 'voting') -> Tuple[np.ndarray, np.ndarray]:
        """
        Ensemble prediction
        
        Methods:
        - voting: Majority vote
        - averaging: Average probabilities
        - weighted: Weighted average based on validation performance
        """
        
        all_probs = self.predict_probabilities(X)
        
        if method == 'voting':
            predictions = []
            for model in self.models.values():
                predictions.append(model.predict(X))
            predictions = np.array(predictions)
            # Majority vote
            final_pred = np.apply_along_axis(
                lambda x: np.bincount(x).argmax(), 
                axis=0, 
                arr=predictions
            )
            final_probs = None
            
        elif method == 'averaging':
            # Average probabilities
            probs_stack = np.array(list(all_probs.values()))
            final_probs = np.mean(probs_stack, axis=0)
            final_pred = np.argmax(final_probs, axis=1)
            
        elif method == 'weighted':
            # Weighted average (weights based on validation accuracy)
            weights = self._calculate_weights()
            weighted_probs = np.zeros_like(list(all_probs.values())[0])
            
            for name, probs in all_probs.items():
                weighted_probs += weights[name] * probs
            
            final_probs = weighted_probs
            final_pred = np.argmax(final_probs, axis=1)
        
        return final_pred, final_probs
    
    def _calculate_weights(self) -> Dict[str, float]:
        """Calculate model weights based on performance"""
        # This would use validation set performance
        # For now, equal weights
        n_models = len(self.models)
        return {name: 1.0 / n_models for name in self.models.keys()}


class SeverityPredictor:
    """
    Predicts vulnerability severity (Critical, High, Medium, Low)
    """
    
    def __init__(self):
        self.model = None
    
    def build_model(self):
        """Build severity prediction model"""
        
        self.model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=8,
            learning_rate=0.1,
            objective='multi:softprob',
            random_state=42
        )
    
    def train(self, X: pd.DataFrame, y: pd.Series):
        """Train severity predictor"""
        self.model.fit(X, y)
    
    def predict(self, X: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """Predict severity and confidence"""
        predictions = self.model.predict(X)
        probabilities = self.model.predict_proba(X)
        
        return predictions, probabilities


class ChainDetector:
    """
    Detects vulnerability chains (combinations of vulns that lead to critical impact)
    """
    
    def __init__(self):
        self.chain_patterns = self._build_chain_patterns()
        self.graph = None
    
    def _build_chain_patterns(self) -> List[Dict]:
        """Define known vulnerability chain patterns"""
        
        patterns = [
            {
                'name': 'Account Takeover Chain',
                'vulns': ['INFO_DISCLOSURE', 'CSRF', 'AUTH_BYPASS'],
                'severity': 'critical',
                'description': 'Email enumeration + CSRF + Weak auth'
            },
            {
                'name': 'RCE Chain',
                'vulns': ['FILE_UPLOAD', 'PATH_TRAVERSAL', 'RCE'],
                'severity': 'critical',
                'description': 'File upload + path traversal to RCE'
            },
            {
                'name': 'Data Exfiltration Chain',
                'vulns': ['IDOR', 'INFO_DISCLOSURE', 'SSRF'],
                'severity': 'high',
                'description': 'IDOR + info disclosure + SSRF'
            },
            {
                'name': 'Privilege Escalation Chain',
                'vulns': ['IDOR', 'BUSINESS_LOGIC', 'AUTH_BYPASS'],
                'severity': 'critical',
                'description': 'IDOR + logic flaw + auth bypass'
            }
        ]
        
        return patterns
    
    def detect_chains(self, vulnerabilities: List[str]) -> List[Dict]:
        """Detect if vulnerabilities form a chain"""
        
        detected_chains = []
        
        for pattern in self.chain_patterns:
            if all(vuln in vulnerabilities for vuln in pattern['vulns']):
                detected_chains.append(pattern)
        
        return detected_chains
    
    def calculate_chain_score(self, chain: Dict) -> float:
        """Calculate exploitability score for a chain"""
        
        severity_scores = {
            'critical': 10.0,
            'high': 7.5,
            'medium': 5.0,
            'low': 2.5
        }
        
        base_score = severity_scores.get(chain['severity'], 5.0)
        
        # Adjust based on complexity
        complexity_multiplier = 1.0 / len(chain['vulns'])
        
        return base_score * (1 + complexity_multiplier)
